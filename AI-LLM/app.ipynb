{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tasRxpvcPLxw"
      },
      "outputs": [],
      "source": [
        "# Install all required dependencies\n",
        "!pip install -q streamlit langchain transformers torch bitsandbytes langchain-community langchain-huggingface faiss-cpu sentence-transformers networkx matplotlib PyPDF2 python-dotenv huggingface_hub pyngrok requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACEHUB_API_TOKEN=\"****\""
      ],
      "metadata": {
        "id": "cx7vVhamq_5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat .env"
      ],
      "metadata": {
        "id": "3RdZ14QzvUct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import login, HfApi\n",
        "\n",
        "# Replace with your Hugging Face token and model\n",
        "HF_TOKEN = \"******\"\n",
        "LLM_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Login and check authentication\n",
        "login(token=HF_TOKEN)\n",
        "api = HfApi()\n",
        "user_info = api.whoami(token=HF_TOKEN)\n",
        "print(f\"‚úÖ Authenticated as: {user_info['name']}\")\n",
        "\n",
        "# 4-bit quantization config (for faster, memory-efficient loading)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    padding_side=\"left\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model and tokenizer loaded successfully!\")"
      ],
      "metadata": {
        "id": "Xs8LOUNQdRsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "import textwrap\n",
        "import hashlib\n",
        "import pyperclip\n",
        "from PyPDF2 import PdfReader\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import HfApi, login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "LLM_MODEL = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"\")\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    st.error(\"‚ùå HUGGINGFACEHUB_API_TOKEN not found. Please set it in your environment.\")\n",
        "    st.stop()\n",
        "\n",
        "# Initialize components\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
        ")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "\n",
        "# Initialize session state\n",
        "if 'vector_store' not in st.session_state:\n",
        "    st.session_state.vector_store = None\n",
        "if 'processed' not in st.session_state:\n",
        "    st.session_state.processed = False\n",
        "if 'last_file_hash' not in st.session_state:\n",
        "    st.session_state.last_file_hash = None\n",
        "if 'user_name' not in st.session_state:\n",
        "    st.session_state.user_name = None\n",
        "if 'llm_pipeline' not in st.session_state:\n",
        "    st.session_state.llm_pipeline = None\n",
        "\n",
        "# Structured prompt templates\n",
        "answer_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "[INST] <<SYS>>\n",
        "You are a professional academic assistant analyzing research papers. Structure your answer with these exact section headers:\n",
        "\n",
        "1. KEY CONCEPT: Identify the main concept (1-2 sentences)\n",
        "2. MATHEMATICAL FORMULATION: Provide equations/formulas with explanations\n",
        "3. MATHEMATICAL INTUITION: Explain the meaning and significance\n",
        "4. PRACTICAL IMPLICATIONS: Describe 3-5 applications/benefits\n",
        "5. SUMMARY: Brief 2-3 sentence recap\n",
        "\n",
        "Format equations using $$ for LaTeX and wrap code in ```.\n",
        "<</SYS>>\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\n",
        "[/INST]\n",
        "\"\"\")\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "[INST] <<SYS>>\n",
        "Summarize the following research paper content in 100 words using bullet points\n",
        "<</SYS>>\n",
        "\n",
        "CONTEXT: {context}\n",
        "[/INST]\n",
        "\"\"\")\n",
        "\n",
        "quiz_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "[INST] <<SYS>>\n",
        "Generate 3 true/false questions based on this content with answers explained\n",
        "<</SYS>>\n",
        "\n",
        "CONTEXT: {context}\n",
        "[/INST]\n",
        "\"\"\")\n",
        "\n",
        "def format_structured_response(response):\n",
        "    \"\"\"Convert raw response into formatted markdown with sections\"\"\"\n",
        "    sections = {\n",
        "        \"KEY CONCEPT\": \"\",\n",
        "        \"MATHEMATICAL FORMULATION\": \"\",\n",
        "        \"MATHEMATICAL INTUITION\": \"\",\n",
        "        \"PRACTICAL IMPLICATIONS\": \"\",\n",
        "        \"SUMMARY\": \"\"\n",
        "    }\n",
        "\n",
        "    current_section = None\n",
        "    for line in response.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "\n",
        "        # Detect section headers\n",
        "        if \"KEY CONCEPT:\" in line:\n",
        "            current_section = \"KEY CONCEPT\"\n",
        "            line = line.replace(\"KEY CONCEPT:\", \"\").strip()\n",
        "        elif \"MATHEMATICAL FORMULATION:\" in line:\n",
        "            current_section = \"MATHEMATICAL FORMULATION\"\n",
        "            line = line.replace(\"MATHEMATICAL FORMULATION:\", \"\").strip()\n",
        "        elif \"MATHEMATICAL INTUITION:\" in line:\n",
        "            current_section = \"MATHEMATICAL INTUITION\"\n",
        "            line = line.replace(\"MATHEMATICAL INTUITION:\", \"\").strip()\n",
        "        elif \"PRACTICAL IMPLICATIONS:\" in line:\n",
        "            current_section = \"PRACTICAL IMPLICATIONS\"\n",
        "            line = line.replace(\"PRACTICAL IMPLICATIONS:\", \"\").strip()\n",
        "        elif \"SUMMARY:\" in line:\n",
        "            current_section = \"SUMMARY\"\n",
        "            line = line.replace(\"SUMMARY:\", \"\").strip()\n",
        "\n",
        "        if current_section and line:\n",
        "            sections[current_section] += line + \"\\n\\n\"\n",
        "\n",
        "    # Generate formatted markdown\n",
        "    formatted = \"\"\"\n",
        "## Research Paper Analysis\n",
        "\n",
        "### üîë Key Concept\n",
        "{key_concept}\n",
        "\n",
        "### üìê Mathematical Formulation\n",
        "{math_formulation}\n",
        "\n",
        "### üí° Mathematical Intuition\n",
        "{math_intuition}\n",
        "\n",
        "### üöÄ Practical Implications\n",
        "{practical_implications}\n",
        "\n",
        "### üìù Summary\n",
        "{summary}\n",
        "\"\"\".format(\n",
        "        key_concept=sections[\"KEY CONCEPT\"] or \"Not specified\",\n",
        "        math_formulation=sections[\"MATHEMATICAL FORMULATION\"] or \"No equations provided\",\n",
        "        math_intuition=sections[\"MATHEMATICAL INTUITION\"] or \"No intuition provided\",\n",
        "        practical_implications=sections[\"PRACTICAL IMPLICATIONS\"] or \"No implications provided\",\n",
        "        summary=sections[\"SUMMARY\"] or \"No summary provided\"\n",
        "    )\n",
        "\n",
        "    return formatted\n",
        "\n",
        "def get_file_hash(uploaded_file):\n",
        "    \"\"\"Generate hash for file content\"\"\"\n",
        "    hasher = hashlib.sha256()\n",
        "    hasher.update(uploaded_file.getvalue())\n",
        "    return hasher.hexdigest()\n",
        "\n",
        "def process_files(uploaded_files):\n",
        "    \"\"\"Process uploaded PDF files and create vector store\"\"\"\n",
        "    try:\n",
        "        all_text = \"\"\n",
        "        for uploaded_file in uploaded_files:\n",
        "            # Check if file has changed\n",
        "            current_hash = get_file_hash(uploaded_file)\n",
        "            if st.session_state.last_file_hash == current_hash:\n",
        "                st.info(\"File already processed. Using cached data.\")\n",
        "                return True\n",
        "            st.session_state.last_file_hash = current_hash\n",
        "\n",
        "            # Read PDF\n",
        "            pdf_reader = PdfReader(uploaded_file)\n",
        "            for page in pdf_reader.pages:\n",
        "                all_text += page.extract_text() or \"\"\n",
        "\n",
        "        if not all_text.strip():\n",
        "            st.error(\"‚ùå No text extracted from PDFs\")\n",
        "            return False\n",
        "\n",
        "        # Split text\n",
        "        chunks = text_splitter.split_text(all_text)\n",
        "\n",
        "        # Create vector store\n",
        "        with st.spinner(\"üîß Creating knowledge base...\"):\n",
        "            vectorstore = FAISS.from_texts(chunks, embeddings)\n",
        "            st.session_state.vector_store = vectorstore\n",
        "            st.success(f\"‚úÖ Processed {len(chunks)} document chunks\")\n",
        "            return True\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå File processing failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def initialize_model():\n",
        "    if st.session_state.llm_pipeline is not None:\n",
        "        return True\n",
        "\n",
        "    try:\n",
        "        login(token=HF_TOKEN, add_to_git_credential=True)\n",
        "        os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
        "\n",
        "        # Verify token\n",
        "        api = HfApi()\n",
        "        user_info = api.whoami(token=HF_TOKEN)\n",
        "        st.session_state.user_name = user_info['name']\n",
        "        st.sidebar.success(f\"üîë Authenticated as: {user_info['name']}\")\n",
        "\n",
        "        # Load model\n",
        "        with st.spinner(\"üß† Loading AI model (this may take a few minutes)...\"):\n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "\n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                LLM_MODEL,\n",
        "                padding_side=\"left\",\n",
        "                token=HF_TOKEN\n",
        "            )\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                LLM_MODEL,\n",
        "                quantization_config=bnb_config,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "                token=HF_TOKEN\n",
        "            )\n",
        "\n",
        "            text_generation_pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.5,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                do_sample=True,\n",
        "                return_full_text=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            st.session_state.llm_pipeline = text_generation_pipe\n",
        "            st.success(\"‚úÖ Model loaded successfully!\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Model initialization failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def process_query(question):\n",
        "    if not st.session_state.vector_store:\n",
        "        st.error(\"No documents uploaded\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        retriever = st.session_state.vector_store.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\"k\": 5}\n",
        "        )\n",
        "        docs = retriever.invoke(question)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        formatted_prompt = answer_prompt.format(context=context, question=question)\n",
        "\n",
        "        with st.spinner(\"üîç Analyzing and structuring answer...\"):\n",
        "            response = st.session_state.llm_pipeline(\n",
        "                formatted_prompt,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.5\n",
        "            )\n",
        "            raw_answer = response[0]['generated_text']\n",
        "            return format_structured_response(raw_answer)\n",
        "    except Exception as e:\n",
        "        st.error(f\"‚ùå Query failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_summary():\n",
        "    \"\"\"Generate document summary\"\"\"\n",
        "    if not st.session_state.vector_store:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Get representative content\n",
        "        docs = st.session_state.vector_store.similarity_search(\"summary\", k=5)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        formatted_prompt = summary_prompt.format(context=context)\n",
        "\n",
        "        with st.spinner(\"üìù Generating summary...\"):\n",
        "            response = st.session_state.llm_pipeline(\n",
        "                formatted_prompt,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.3\n",
        "            )\n",
        "            return response[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        st.error(f\"Summary generation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_quiz():\n",
        "    \"\"\"Generate quiz questions\"\"\"\n",
        "    if not st.session_state.vector_store:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Get representative content\n",
        "        docs = st.session_state.vector_store.similarity_search(\"key concepts\", k=5)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        formatted_prompt = quiz_prompt.format(context=context)\n",
        "\n",
        "        with st.spinner(\"‚ùì Generating quiz questions...\"):\n",
        "            response = st.session_state.llm_pipeline(\n",
        "                formatted_prompt,\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            return response[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        st.error(f\"Quiz generation failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"Research Paper Q&A Assistant\",\n",
        "        layout=\"wide\",\n",
        "        page_icon=\"üìö\"\n",
        "    )\n",
        "\n",
        "    # Custom CSS\n",
        "    st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .section {\n",
        "        padding: 15px;\n",
        "        border-radius: 10px;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "    .key-concept {\n",
        "        background-color: #f0f8ff;\n",
        "        border-left: 5px solid #1e90ff;\n",
        "    }\n",
        "    .math-formula {\n",
        "        background-color: #f5f5f5;\n",
        "        border-left: 5px solid #696969;\n",
        "    }\n",
        "    .implications {\n",
        "        background-color: #f0fff0;\n",
        "        border-left: 5px solid #2e8b57;\n",
        "    }\n",
        "    .summary {\n",
        "        background-color: #fffaf0;\n",
        "        border-left: 5px solid #ff8c00;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #4CAF50;\n",
        "        color: white;\n",
        "        border-radius: 5px;\n",
        "        padding: 0.5rem 1rem;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    st.title(\"üìö Research Paper Q&A Assistant\")\n",
        "\n",
        "    # Initialize model\n",
        "    if not initialize_model():\n",
        "        st.stop()\n",
        "\n",
        "    # File Upload\n",
        "    with st.sidebar:\n",
        "        st.header(\"üìÑ Document Input\")\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"Upload PDF research papers\",\n",
        "            type=\"pdf\",\n",
        "            accept_multiple_files=True,\n",
        "            help=\"Upload one or more research papers in PDF format\"\n",
        "        )\n",
        "\n",
        "        if uploaded_files:\n",
        "            if st.button(\"Process PDFs\", key=\"process_pdfs\", type=\"primary\"):\n",
        "                with st.spinner(\"Processing documents...\"):\n",
        "                    if process_files(uploaded_files):\n",
        "                        st.session_state.processed = True\n",
        "                        st.rerun()\n",
        "\n",
        "        st.divider()\n",
        "        url_input = st.text_input(\"üåê Or enter a research paper URL:\", placeholder=\"https://arxiv.org/pdf/...\")\n",
        "        if url_input:\n",
        "            if st.button(\"Process URL\", key=\"process_url\", type=\"primary\"):\n",
        "                with st.spinner(\"Processing URL...\"):\n",
        "                    try:\n",
        "                        loader = WebBaseLoader(url_input)\n",
        "                        docs = loader.load()\n",
        "                        if not docs or not docs[0].page_content.strip():\n",
        "                            st.error(\"‚ùå No content loaded from the provided URL\")\n",
        "                        else:\n",
        "                            chunks = text_splitter.split_documents(docs)\n",
        "                            vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "                            st.session_state.vector_store = vectorstore\n",
        "                            st.session_state.processed = True\n",
        "                            st.success(f\"‚úÖ Processed {len(chunks)} document chunks\")\n",
        "                            st.rerun()\n",
        "                    except Exception as e:\n",
        "                        st.error(f\"‚ùå URL processing failed: {str(e)}\")\n",
        "\n",
        "    # Main content area\n",
        "    if st.session_state.get('processed', False):\n",
        "        st.success(\"‚úÖ Documents ready for analysis\")\n",
        "\n",
        "        # Question Answering\n",
        "        st.header(\"‚ùì Ask About Your Paper\")\n",
        "        user_question = st.text_area(\n",
        "            \"Enter your question about the research paper:\",\n",
        "            placeholder=\"Explain the main methodology or mathematical intuition...\",\n",
        "            height=120,\n",
        "            key=\"question_input\"\n",
        "        )\n",
        "\n",
        "        if st.button(\"Get Answer\", type=\"primary\"):\n",
        "            answer = process_query(user_question)\n",
        "            if answer:\n",
        "                st.subheader(\"Analysis Results\")\n",
        "                st.markdown(answer, unsafe_allow_html=True)\n",
        "\n",
        "                # Copy to clipboard\n",
        "                if st.button(\"üìã Copy to Clipboard\", key=\"copy_btn\"):\n",
        "                    pyperclip.copy(answer)\n",
        "                    st.toast(\"Answer copied to clipboard!\", icon=\"‚úì\")\n",
        "\n",
        "        # Analysis Tools\n",
        "        st.divider()\n",
        "        st.header(\"üîç Analysis Tools\")\n",
        "\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            st.subheader(\"üìù Summary Generator\")\n",
        "            if st.button(\"Generate Summary\", key=\"summary_btn\", use_container_width=True):\n",
        "                summary = generate_summary()\n",
        "                if summary:\n",
        "                    st.markdown(f\"\"\"\n",
        "                    <div class=\"section summary\">\n",
        "                        {textwrap.fill(summary, width=80)}\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "        with col2:\n",
        "            st.subheader(\"‚ùì Quiz Generator\")\n",
        "            if st.button(\"Generate Quiz\", key=\"quiz_btn\", use_container_width=True):\n",
        "                quiz = generate_quiz()\n",
        "                if quiz:\n",
        "                    st.markdown(f\"\"\"\n",
        "                    <div class=\"section\">\n",
        "                        {textwrap.fill(quiz, width=80)}\n",
        "                    </div>\n",
        "                    \"\"\", unsafe_allow_html=True)\n",
        "    else:\n",
        "        st.info(\"‚ÑπÔ∏è Please upload PDF research papers or enter a URL to begin analysis\")\n",
        "        st.image(\"https://images.unsplash.com/photo-1506880018603-83d5b814b5a6?auto=format&fit=crop&w=1200\",\n",
        "                 caption=\"Academic Research\", use_column_width=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "EQYwKDVUPR0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cell 4: Set up ngrok and run Streamlit\n",
        "!pip install -q pyngrok\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "# Kill existing ngrok and Streamlit processes\n",
        "!pkill ngrok\n",
        "!pkill -f streamlit\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(\"2vaEHf6f3GHr6NuvDwbixSfPqp6_5CgasErmF2bLCbPozsGzF\")  # Your ngrok token\n",
        "\n",
        "# Run Streamlit in background\n",
        "def run_streamlit():\n",
        "    subprocess.run(\n",
        "        [\"nohup\", \"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\", \"--browser.gatherUsageStats\", \"false\"],\n",
        "        stdout=open('streamlit.log', 'w'),\n",
        "        stderr=subprocess.STDOUT\n",
        "    )\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to initialize\n",
        "time.sleep(8)\n",
        "\n",
        "# Set up ngrok tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(8501, \"http\")\n",
        "    print(\"\\n‚≠ê Your app is now available at:\", public_url)\n",
        "    while True:\n",
        "        time.sleep(3600)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüö´ Shutting down...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {str(e)}\")\n",
        "finally:\n",
        "    ngrok.kill()\n",
        "    print(\"‚úÖ Ngrok tunnel closed\")\n",
        "    !pkill -f streamlit\n",
        "    print(\"‚úÖ Streamlit server stopped\")"
      ],
      "metadata": {
        "id": "8DMN25H2Voo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_Yq7t7IlGP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "with open(\"/content/NIPS-2017-attention-is-all-you-need-Paper.pdf\", \"rb\") as f:\n",
        "    pdf_reader = PdfReader(f)\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    print(f\"Extracted {len(text)} characters\")"
      ],
      "metadata": {
        "id": "yL1_wAdSEqCj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
